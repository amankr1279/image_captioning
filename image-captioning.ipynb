{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Package Imports","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model \nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"execution":{"iopub.status.busy":"2023-09-06T19:11:40.777367Z","iopub.execute_input":"2023-09-06T19:11:40.777726Z","iopub.status.idle":"2023-09-06T19:11:50.520509Z","shell.execute_reply.started":"2023-09-06T19:11:40.777685Z","shell.execute_reply":"2023-09-06T19:11:50.519413Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2023-09-06T19:11:50.524351Z","iopub.execute_input":"2023-09-06T19:11:50.524895Z","iopub.status.idle":"2023-09-06T19:11:50.529569Z","shell.execute_reply.started":"2023-09-06T19:11:50.524866Z","shell.execute_reply":"2023-09-06T19:11:50.528591Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Extract Image Features","metadata":{}},{"cell_type":"code","source":"model = VGG16()\n# leaving out the predeiction layer\nmodel = Model(inputs = model.inputs, outputs=model.layers[-2].output) \nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-09-06T19:11:50.530820Z","iopub.execute_input":"2023-09-06T19:11:50.531793Z","iopub.status.idle":"2023-09-06T19:11:59.966642Z","shell.execute_reply.started":"2023-09-06T19:11:50.531757Z","shell.execute_reply":"2023-09-06T19:11:59.965875Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467096/553467096 [==============================] - 2s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n fc1 (Dense)                 (None, 4096)              102764544 \n                                                                 \n fc2 (Dense)                 (None, 4096)              16781312  \n                                                                 \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"features = {}\ndirectory = os.path.join(BASE_DIR, 'Images')\n\nfor img_name in tqdm(os.listdir(directory)):\n    # load image and make it numpy array\n    img_path = directory + '/'+ img_name\n    image = load_img(img_path, target_size=(224,224))\n    image = img_to_array(image) \n    # reshape and preprocess for vgg16 \n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    image = preprocess_input(image)\n    # store feature\n    extracted_feature = model.predict(image, verbose=0)\n    image_id = img_name.split('.')[0]\n    features[image_id] = extracted_feature\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-06T19:11:59.967673Z","iopub.execute_input":"2023-09-06T19:11:59.968047Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc002cdff8254f7bb6c75ca568f0aab8"}},"metadata":{}}]},{"cell_type":"code","source":"# store features in file for reference\npickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load features from pickle dump\nwith open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load captions","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'r')as f:\n    next(f)\n    captions_doc = f.read()\ncaptions_doc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions = {}\nfor line in tqdm(captions_doc.split('\\n')):\n    tokens = line.split(',')\n    if len(line) < 2:\n        continue\n    image_id, caption = tokens[0], tokens[1:]\n    image_id = image_id.split('.')[0]\n    caption = \" \".join(caption)\n    if image_id not in captions:\n        captions[image_id] = []\n    captions[image_id].append(caption)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Data","metadata":{}},{"cell_type":"code","source":"print(captions['1000268201_693b08cb0e'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(img_captions_map):\n    for img_id, caption_list in img_captions_map.items():\n        for i in range(len(caption_list)):\n            # remove non-alphabetical characters\n            cap = caption_list[i]\n            cap = cap.lower().replace('\\s+','').replace('[^A-Za-z]','')\n            cap = \" \".join([word for word in cap.split() if len(word)>1])\n            # add start and end tags\n            cap = '<start> ' + cap + ' <end>'\n            caption_list[i] = cap\nclean(captions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(captions['1000268201_693b08cb0e'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_captions = []\nfor key in captions:\n    for cap in captions[key]:\n        all_captions.append(cap)\nprint(len(all_captions))\nall_captions[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index)+1\nmax_length = max(len(caption.split()) for caption in all_captions) # for padding purposes\nprint(vocab_size)\nprint(max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Test Data","metadata":{}},{"cell_type":"code","source":"image_ids = list(captions.keys())\nsplit = int(len(image_ids) * 0.90)\ntrain,test = image_ids[:split], image_ids[split:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batched Input","metadata":{}},{"cell_type":"code","source":"def data_generator(data_keys, caption_map, tokenizer, voacb_size, batch_size):\n    X1, X2, Y = list(), list(), list()\n    n = 0\n    while True:\n        for key in data_keys:\n            n += 1\n            caption_list = caption_map[key]\n            for cap in caption_list:\n                seq = tokenizer.texts_to_sequences([cap])[0]\n                ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}